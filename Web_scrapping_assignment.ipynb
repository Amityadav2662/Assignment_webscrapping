{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63d309-e692-4eab-8800-46aad8db95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "Ans.\n",
    "Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured \n",
    "data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be\n",
    "used in various applications.\n",
    "\n",
    "Web scraping is the process of using to extract content and data from a website. \n",
    "The three areas where web scrapping is used to get data.\n",
    "1.Monitoring e-commerce prices\n",
    "2.Analyzing social media web data\n",
    "3.Applying machine learning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29507ace-437a-4c07-8f77-87e840b16855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.What are the different methods used for Web Scraping?\n",
    "Ans. \n",
    "There are the different methods used for web scraping\n",
    "\n",
    "1.COPY-PASTING: When we copy and paste manullay from the websites into a local file and spreadsheats.It is very time consuming\n",
    "task to do so.\n",
    "2.HTML PARSING: HTML parsing involves using libraries or tools like Beautiful Soup to parse the HTML structure of a web page.\n",
    "These libraries provide methods to navigate and extract data from the parsed HTML, making it easier to locate\n",
    "and retrieve desired elements.\n",
    "3.XPATH: XML Path Language is a query language that is used with XML documents. XPath can be used to navigate XML documents\n",
    "because of their tree-like structure by selecting nodes based on different parameters.\n",
    "4.GOOGLE SHEETS: Google sheets are a web scraping tool that is quite popular among web scrapers. From within sheets, a scraper\n",
    "can make use of import XML (,) function to scrape as much data as is needed from websites.\n",
    "5.DOM PARSING: DOM is short for Document Object Model and it defines the style structure and content of XML files. Scrapers make\n",
    "use of DOM parsers to get an in-depth view of a web page’s structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a65483-16d1-4fde-85f5-3b74e64c5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is Beautiful Soup? Why is it used?\n",
    "Ans.\n",
    "Beautiful Soup is a Python library that makes it easy to scrape information from web pages. It sits a top an HTML or XML parser\n",
    "and provides Pythonic idioms for iterating, searching, and modifying the parse tree.\n",
    "The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from \n",
    "HTML tags, and alter the HTML in the document with which we’re working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371775c-7a51-4010-8671-827b3a82d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Why is flask used in this Web Scraping project?\n",
    "Ans.\n",
    "Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML \n",
    "in a new HTML file. The requests module allows us to send http requests to the website we want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df95751-ffba-4b9c-8dae-a04a7bc27956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "Ans.\n",
    "In this project We are using two aws services to first is code pipeline and the second one is Amazon Elastic Beanstalk.\n",
    "the code pipeline integrat the Elastic Beanstalk.\n",
    "\n",
    "The use of the services:\n",
    "Codepipeline: AWS CodePipeline is a used for continuous delivery service you can use to model, visualize, and automate the\n",
    "steps required to release your software. You can quickly model and configure the different stages of a software release\n",
    "process. CodePipeline automates the steps required to release your software changes continuously.\n",
    "\n",
    "Amazon Elastic Beanstalk: Elastic Beanstalk is a service for deploying and scaling web applications and services. Upload your \n",
    "code and Elastic Beanstalk automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling\n",
    "to application health monitoring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
